{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DomainBed.domainbed.lib.fast_data_loader import InfiniteDataLoader, FastDataLoader\n",
    "from DomainBed.domainbed.datasets import get_dataset_class\n",
    "from networks import CMNIST_MLP, Classifier, ClassifierV2, MMDClassifier\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from utils import AverageMeter, compute_acc\n",
    "from dataloader_factory import get_kfold_cross_validation_splits\n",
    "\n",
    "n_repeats = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'data_augmentation': False,\n",
    "    'cmnist_env_ps': [0.1, 0.1], #args.cmnist_env_ps,\n",
    "    'cmnist_blue_means': [0.0, 0.0], #args.cmnist_blue_means,\n",
    "    'cmnist_blue_stds': [0.0, 0.0], #args.cmnist_blue_stds,\n",
    "}\n",
    "dataset_class = get_dataset_class('ColoredMNIST_IRM_Blue')\n",
    "dataset = dataset_class('/home/ma-user/work/kaican/dataset', [], hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'data_augmentation': False,\n",
    "    'cmnist_env_ps': [0.5, 0.5], #args.cmnist_env_ps,\n",
    "    'cmnist_blue_means': [0.0, 1.0], #args.cmnist_blue_means,\n",
    "    'cmnist_blue_stds': [0.1, 0.1], #args.cmnist_blue_stds,\n",
    "}\n",
    "dataset_class = get_dataset_class('ColoredMNIST_IRM_Blue')\n",
    "dataset = dataset_class('/home/ma-user/work/kaican/dataset', [], hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NI(model, envs):\n",
    "    ''' Computes Non-I.I.D. Index (https://arxiv.org/pdf/1906.02899.pdf)\n",
    "    Args:\n",
    "        model: an ERM model predicting y1.\n",
    "        dataset: a dataset.\n",
    "    Returns:\n",
    "        The Non-I.I.D. Index.\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    embs = [[], []]\n",
    "    for i, env in enumerate(envs):\n",
    "        data = [env[i] for i in range(len(env))]\n",
    "        x = torch.stack([p[0] for p in data]).cuda()\n",
    "        y = torch.stack([p[1] for p in data]).cuda()\n",
    "        embs[i].append(model.backbone(x).data.cpu().numpy())\n",
    "    embs_tr = np.concatenate(embs[0])\n",
    "    embs_te = np.concatenate(embs[1])\n",
    "    embs = np.concatenate([embs_tr, embs_te])\n",
    "    \n",
    "    mean_tr = embs_tr.mean(0)\n",
    "    mean_te = embs_te.mean(0)\n",
    "    std_emb = embs.std(0)\n",
    "    \n",
    "    normalized = (mean_tr - mean_te) / (std_emb + 1e-8)\n",
    "    return np.linalg.norm(normalized, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NI\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "n_steps = 1000\n",
    "lr = 0.01\n",
    "\n",
    "splits = get_kfold_cross_validation_splits(dataset, n_repeats, seed=0)\n",
    "\n",
    "results = []\n",
    "for k, (tr_envs, vl_envs) in enumerate(splits):\n",
    "    model = Classifier(CMNIST_MLP(), 2).cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=0.0005)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_steps)\n",
    "\n",
    "    tr_env = tr_envs[0]\n",
    "    vl_env = vl_envs[0]\n",
    "\n",
    "\n",
    "    tr_loader = InfiniteDataLoader(tr_env, None, batch_size, num_workers)\n",
    "    tr_iter = iter(tr_loader)\n",
    "\n",
    "    vl_loader = FastDataLoader(vl_env, 128, 16)\n",
    "\n",
    "    for i in range(1, n_steps + 1):\n",
    "        x, y = next(tr_iter)\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not i % 200:\n",
    "            model.eval()\n",
    "\n",
    "            acc = AverageMeter()\n",
    "            with torch.no_grad():\n",
    "                for x, y in vl_loader:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                    logits = model(x)\n",
    "                    acc.update(compute_acc(logits, y), x.size(0))\n",
    "\n",
    "            print(f'step {i}:')\n",
    "            print(f'loss {loss.item():.4f}\\tacc: {acc.avg:6.3f}')\n",
    "\n",
    "            model.train()\n",
    "\n",
    "    r = compute_NI(model, vl_envs)\n",
    "    print(r)\n",
    "    results.append(r)\n",
    "\n",
    "arr = np.array(results)\n",
    "print(arr.mean(), arr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "import ot.plot\n",
    "\n",
    "\n",
    "def compute_EMD(xs, xt):\n",
    "    ''' Earth mover's distance, a.k.a. Wasserstein metric.\n",
    "    Args:\n",
    "        xs: points from source domain.\n",
    "        xt: points from target domain.\n",
    "    Returns:\n",
    "        Earth mover's distance.\n",
    "    '''\n",
    "    assert xs.shape[0] == xt.shape[0]\n",
    "    n = xs.shape[0]\n",
    "    \n",
    "    M = ot.dist(xs, xt)\n",
    "    M /= M.max()\n",
    "    \n",
    "    a, b = np.ones((n,)) / n, np.ones((n,)) / n\n",
    "    G0 = ot.emd(a, b, M, numItermax=1000000)\n",
    "    \n",
    "    return np.sum(G0 * M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMD\n",
    "results = []\n",
    "\n",
    "for i in range(n_repeats):\n",
    "    r = compute_EMD(dataset[0][i::n_repeats][0].view(6000, -1).data.numpy(),\n",
    "                    dataset[1][i::n_repeats][0].view(6000, -1).data.numpy())\n",
    "    results.append(r)\n",
    "arr = np.array(results)\n",
    "print(arr.mean(), arr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MMD(model, envs):\n",
    "    ''' Maximum mean discrepancy (https://papers.nips.cc/paper/2006/file/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Paper.pdf)\n",
    "    Args:\n",
    "        model: an ERM model predicting either an example is from training or test environments.\n",
    "        envs: training and test environments.\n",
    "    Returns:\n",
    "        Maximum mean discrepancy.\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    pred1 = AverageMeter()\n",
    "    pred2 = AverageMeter()\n",
    "    \n",
    "    for i, batch in enumerate(envs, 1):\n",
    "        n_envs = len(batch)\n",
    "        for j, env in enumerate(batch):\n",
    "            if j+1 < n_envs:\n",
    "                x = env['images'][::n_envs-1].cuda()\n",
    "                y1 = env['labels'][::n_envs-1].squeeze(1).cuda()\n",
    "                pred = pred1\n",
    "            else:\n",
    "                x = env['images'].cuda()\n",
    "                y1 = env['labels'].squeeze(1).cuda()\n",
    "                pred = pred2\n",
    "            \n",
    "            logits = model(x)\n",
    "            pred_ = torch.sigmoid(logits)\n",
    "            pred.update(pred_.mean(), x.size(0))\n",
    "    \n",
    "    return abs(pred1.avg - pred2.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMD\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "n_steps = 1000\n",
    "lr = 0.01\n",
    "\n",
    "splits = get_kfold_cross_validation_splits(dataset, n_repeats, seed=0)\n",
    "\n",
    "results = []\n",
    "for k, (envs, vl_envs) in enumerate(splits):\n",
    "    model = MMDClassifier(CMNIST_MLP()).cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=0.0005)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_steps)\n",
    "\n",
    "    tr_env, te_env = envs\n",
    "    vl_tr_env, vl_te_env = vl_envs\n",
    "    \n",
    "    tr_loader = InfiniteDataLoader(tr_env, None, batch_size, num_workers)\n",
    "    tr_iter = iter(tr_loader)\n",
    "    \n",
    "    te_loader = InfiniteDataLoader(te_env, None, batch_size, num_workers)\n",
    "    te_iter = iter(te_loader)\n",
    "\n",
    "    vl_tr_loader = FastDataLoader(vl_tr_env, 64, 8)\n",
    "    vl_te_loader = FastDataLoader(vl_te_env, 64, 8)\n",
    "\n",
    "    for i in range(1, n_steps + 1):\n",
    "        tr_x, _ = next(tr_iter)\n",
    "        te_x, _ = next(te_iter)\n",
    "        \n",
    "        x = torch.cat([tr_x, te_x]).cuda()\n",
    "        e = torch.cat([torch.zeros(tr_x.size(0)), torch.ones(tr_x.size(0))]).cuda()\n",
    "        e.unsqueeze_(1)\n",
    "        \n",
    "        logits = model(x)\n",
    "        pred = torch.sigmoid(logits)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, e)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i == 1 or not i % 200:\n",
    "            model.eval()\n",
    "\n",
    "            tr_meter = AverageMeter()\n",
    "            te_meter = AverageMeter()\n",
    "            loss = AverageMeter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for (tr_x, _), (te_x, _) in zip(vl_tr_loader, vl_te_loader):\n",
    "                    tr_x = tr_x.cuda()\n",
    "                    te_x = te_x.cuda()\n",
    "                    \n",
    "                    tr_e = torch.zeros(tr_x.size(0)).unsqueeze_(1).cuda()\n",
    "                    te_e = torch.ones(te_x.size(0)).unsqueeze_(1).cuda()\n",
    "                    \n",
    "                    tr_logits = model(tr_x)\n",
    "                    tr_pred = torch.sigmoid(tr_logits)\n",
    "                    loss.update(F.binary_cross_entropy_with_logits(tr_pred, tr_e), tr_e.size(0))\n",
    "                    \n",
    "                    te_logits = model(te_x)\n",
    "                    te_pred = torch.sigmoid(te_logits)\n",
    "                    loss.update(F.binary_cross_entropy_with_logits(te_pred, te_e), te_e.size(0))\n",
    "                    \n",
    "                    tr_meter.update(tr_pred.data.cpu().numpy().mean(), tr_x.size(0))\n",
    "                    te_meter.update(te_pred.data.cpu().numpy().mean(), te_x.size(0))\n",
    "\n",
    "            print(f'step {i}:')\n",
    "            print(f'loss {loss.avg:.4f}')\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            if i == n_steps:\n",
    "                results.append(abs(tr_meter.avg - te_meter.avg))\n",
    "\n",
    "#     r = compute_MMD(model, vl_envs)\n",
    "#     print(r)\n",
    "#     results.append(r)\n",
    "\n",
    "arr = np.array(results)\n",
    "print(arr.mean(), arr.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
